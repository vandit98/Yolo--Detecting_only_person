{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Installing dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T14:23:34.544846Z","iopub.status.busy":"2023-11-15T14:23:34.543963Z","iopub.status.idle":"2023-11-15T14:23:56.731832Z","shell.execute_reply":"2023-11-15T14:23:56.730930Z","shell.execute_reply.started":"2023-11-15T14:23:34.544811Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Ultralytics YOLOv8.0.209 🚀 Python-3.10.10 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","Setup complete ✅ (4 CPUs, 31.4 GB RAM, 5122.4/8062.4 GB disk)\n"]}],"source":["%pip install ultralytics\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"markdown","metadata":{},"source":["## Creating data.yaml file "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T14:23:56.734704Z","iopub.status.busy":"2023-11-15T14:23:56.733769Z","iopub.status.idle":"2023-11-15T14:23:56.739675Z","shell.execute_reply":"2023-11-15T14:23:56.738812Z","shell.execute_reply.started":"2023-11-15T14:23:56.734661Z"},"trusted":true},"outputs":[],"source":["config_file_template = '''\n","train: ./Face_Detection/train/images\n","val: ./Face_Detection/valid/images\n","test: ./Face_Detection/test/images\n","\n","nc: 1\n","names: ['face']\n","\n","\n","'''\n","\n","with open('data.yaml', 'w') as f:\n","    f.write(config_file_template)"]},{"cell_type":"markdown","metadata":{},"source":["## Training yolo model "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T12:59:31.726655Z","iopub.status.busy":"2023-11-15T12:59:31.726208Z","iopub.status.idle":"2023-11-15T13:11:19.038929Z","shell.execute_reply":"2023-11-15T13:11:19.037919Z","shell.execute_reply.started":"2023-11-15T12:59:31.726625Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n","100%|███████████████████████████████████████| 21.5M/21.5M [00:00<00:00, 196MB/s]\n","Ultralytics YOLOv8.0.209 🚀 Python-3.10.10 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/kaggle/working/data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","100%|████████████████████████████████████████| 755k/755k [00:00<00:00, 21.3MB/s]\n","/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n","  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n","  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n","  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n"," 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n","Model summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n","\n","Transferred 349/355 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n","100%|██████████████████████████████████████| 6.23M/6.23M [00:00<00:00, 99.5MB/s]\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/person-detection/train/labels... 2820 images, 117 \u001b[0m\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/person-detection/train is not writeable, cache not saved.\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/person-detection/valid/labels... 392 images, 19 back\u001b[0m\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/person-detection/valid is not writeable, cache not saved.\n","Plotting labels to runs/detect/train/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 4 dataloader workers\n","Logging results to \u001b[1mruns/detect/train\u001b[0m\n","Starting training for 10 epochs...\n","Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       1/10       4.7G      1.493      1.931      1.571         12        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.441      0.395      0.359      0.164\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       2/10      4.75G      1.648      1.637      1.715         11        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.493      0.355      0.387      0.176\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       3/10      4.76G      1.615        1.6       1.71          9        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.577      0.533      0.548      0.269\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       4/10      4.78G      1.498       1.42      1.605         12        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.744      0.679      0.752      0.413\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       5/10      4.77G      1.455      1.291      1.567          7        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.739      0.617       0.69      0.387\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       6/10      4.77G      1.356      1.189      1.479          6        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.824      0.683      0.788      0.479\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       7/10      4.73G      1.276      1.053      1.414         10        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.812      0.683      0.787      0.467\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       8/10      4.76G      1.216     0.9298      1.369          7        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.826      0.765      0.826      0.527\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       9/10      4.77G      1.143     0.8442      1.318          5        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.871      0.788      0.871      0.581\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      10/10      4.78G      1.076       0.76      1.259          4        640: 1\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.883       0.78      0.889      0.584\n","\n","10 epochs completed in 0.176 hours.\n","Optimizer stripped from runs/detect/train/weights/last.pt, 22.5MB\n","Optimizer stripped from runs/detect/train/weights/best.pt, 22.5MB\n","\n","Validating runs/detect/train/weights/best.pt...\n","Ultralytics YOLOv8.0.209 🚀 Python-3.10.10 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.881      0.781      0.889      0.583\n","Speed: 0.6ms preprocess, 5.6ms inference, 0.0ms loss, 0.8ms postprocess per image\n","Results saved to \u001b[1mruns/detect/train\u001b[0m\n","💡 Learn more at https://docs.ultralytics.com/modes/train\n","\u001b[0m"]}],"source":["!yolo train model=yolov8s.pt data=\"/kaggle/working/data.yaml\" epochs=10"]},{"cell_type":"markdown","metadata":{},"source":["## Validating the reults"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:17:07.840014Z","iopub.status.busy":"2023-11-15T13:17:07.839024Z","iopub.status.idle":"2023-11-15T13:17:26.380297Z","shell.execute_reply":"2023-11-15T13:17:26.379152Z","shell.execute_reply.started":"2023-11-15T13:17:07.839976Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.0.209 🚀 Python-3.10.10 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/person-detection/valid/labels... 392 images, 19 back\u001b[0m\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/person-detection/valid is not writeable, cache not saved.\n","                 Class     Images  Instances      Box(P          R      mAP50  m\n","                   all        392        676      0.886       0.78      0.888      0.584\n","/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","Speed: 0.6ms preprocess, 5.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n","Results saved to \u001b[1mruns/detect/val2\u001b[0m\n","💡 Learn more at https://docs.ultralytics.com/modes/val\n"]}],"source":["!yolo task=detect mode=val model=\"/kaggle/working/runs/detect/train/weights/best.pt\" data=data.yaml"]},{"cell_type":"markdown","metadata":{},"source":["## Inferencing"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:20:05.362248Z","iopub.status.busy":"2023-11-15T13:20:05.361283Z","iopub.status.idle":"2023-11-15T13:20:14.333526Z","shell.execute_reply":"2023-11-15T13:20:14.332317Z","shell.execute_reply.started":"2023-11-15T13:20:05.362207Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.0.209 🚀 Python-3.10.10 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","\n","image 1/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-102_jpg.rf.63f729c6eabeb4e5d1386118457288f1.jpg: 640x640 2 Persons, 8.6ms\n","image 2/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-104_jpg.rf.d9d835681e33fbe70709d5f70e49b1e7.jpg: 640x640 2 Persons, 8.6ms\n","image 3/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-110_jpg.rf.53597e336338b60af2b40d97bacae474.jpg: 640x640 5 Persons, 8.6ms\n","image 4/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-112_jpg.rf.96ccb569974a2865cc582207c3b3039a.jpg: 640x640 3 Persons, 8.6ms\n","image 5/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-113_jpg.rf.df90b722cadd12d4fbe5137241885efe.jpg: 640x640 3 Persons, 8.6ms\n","image 6/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-114_jpg.rf.fecb1b6970249659badebfdfbc5fe51c.jpg: 640x640 5 Persons, 8.6ms\n","image 7/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-118_jpg.rf.317f41bb58c0406c07ba616cca99a608.jpg: 640x640 1 Person, 8.6ms\n","image 8/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-119_jpg.rf.1532f826622c84bfe957e0649cc400f7.jpg: 640x640 2 Persons, 8.5ms\n","image 9/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-131_jpg.rf.872338a4a0e7baa6e192b5673139fa12.jpg: 640x640 4 Persons, 8.6ms\n","image 10/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-139_jpg.rf.568fc9099b82c32c6dce2ef7ce36d21f.jpg: 640x640 2 Persons, 8.6ms\n","image 11/14 /kaggle/input/person-detection/test/images/clip_main_lobby_Tan_mp4-4_jpg.rf.1137880068d9d8c6529e9a0598c6aee1.jpg: 640x640 4 Persons, 8.6ms\n","image 12/14 /kaggle/input/person-detection/test/images/frame_120_jpg.rf.6365c9ed86fd7554aa3c743f6311d78c.jpg: 640x640 4 Persons, 8.6ms\n","image 13/14 /kaggle/input/person-detection/test/images/frame_122_jpg.rf.ca027b695338a23b8fe4849d7944aeb9.jpg: 640x640 4 Persons, 8.5ms\n","image 14/14 /kaggle/input/person-detection/test/images/frame_128_jpg.rf.39feedc1f00f6d12992e4c658e9ad228.jpg: 640x640 4 Persons, 8.6ms\n","Speed: 1.6ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1mruns/detect/predict\u001b[0m\n","💡 Learn more at https://docs.ultralytics.com/modes/predict\n"]}],"source":["!yolo task=detect mode=predict model=\"/kaggle/working/runs/detect/train/weights/best.pt\" conf=0.25 source=\"./Face_Detection/test/images\""]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4002560,"sourceId":6966792,"sourceType":"datasetVersion"}],"dockerImageVersionId":30476,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
